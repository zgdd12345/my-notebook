[链接](https://zhuanlan.zhihu.com/p/1967314577529246001)

垂直领域的SFT微调根本没有必要去做，现在的大模型已经训练过足够的开源知识。检索增强生成RAG如果做得好，就足够了

RAG失败的原因：
1. 索引过多，模型接收的信息太多；
2. 索引过少，数据量太少；
3. 混合存储，非结构化的PPT、PDF、图片和结构化的markdown、word等混合数据embeding。

![[Pasted image 20251101220304.png]]

prompt engineering 已经不是核心了，context engineering 才是下一阶段的主战场。给世界上最聪明的大模型GPT5/claude 4.5喂垃圾输入，它还是会胡说八道。

跑得稳的Agent平台（比如cursor/claude code/codex），都是专注“什么该让模型看、怎么看、以什么形式看”上下了大功夫的，这一点现在应该是共识了

那实际操作中，这些高级的上下文工程到底怎么做？

