[链接](https://zhuanlan.zhihu.com/p/1967314577529246001)

垂直领域的SFT微调根本没有必要去做，现在的大模型已经训练过足够的开源知识。检索增强生成RAG如果做得好，就足够了

RAG失败的原因：
1. 索引过多，模型接收的信息太多；
2. 索引过少，数据量太少；
3. 混合存储，非结构化的PPT、PDF、图片和结构化的markdown、word等混合数据embeding。

![[Pasted image 20251101220304.png]]

prompt engineering 已经不是核心了，context engineering 才是下一阶段的主战场。给世界上最聪明的大模型GPT5/claude 4.5喂垃圾输入，它还是会胡说八道。

跑得稳的Agent平台（比如cursor/claude code/codex），都是专注“什么该让模型看、怎么看、以什么形式看”上下了大功夫的，这一点现在应该是共识了

那实际操作中，这些高级的上下文工程到底怎么做？

## **a) [LLM](https://zhida.zhihu.com/search?content_id=265778318&content_type=Article&match_order=1&q=LLM&zhida_source=entity)的特征选择**

有个老哥换了个角度看这事儿：其实上下文工程就是传统机器学习里的特征工程，只不过换成了大模型版本。以前搞机器学习，特征工程要干这些活：抽取特征、预处理、构造新特征、筛选降维。现在大模型的各种操作，其实对应的就是这些

选择性上下文修剪，相当于特征选择（挑有用的留下）

上下文验证 ，相当于特征预处理（检查格式对不对、类型对不对、是不是过期了）

"上下文可观察性" ，还是特征预处理（追踪哪些输入让结果变好了，哪些让结果变烂了）

带元数据的嵌入增强 ， 相当于特征构造（在原有信息基础上加工出新的）

这个思路挺重要的，把看似新潮的大模型工程，对应到成熟的机器学习方法论上，很多东西就清楚了

### **b) 上下文分层：**

语义+元数据那些已经跑起来的Agent系统，大多采用双层设计

**语义层** → 就是常见的向量搜索
**元数据层** → 按文档类型、时间、访问权限或业务分类来过滤

这种混合结构能统一处理各种乱七八糟的输入（PDF、音频、日志、监控数据），确保Agent不是瞎找"看起来像"的东西，而是真正找到结构化的相关知识。本质上是在向量检索的基础上，叠加分类体系、实体关联和领域约束，让检索结果更贴近业务语义

### **c) [Text-to-SQL](https://zhida.zhihu.com/search?content_id=265778318&content_type=Article&match_order=1&q=Text-to-SQL&zhida_source=entity)能用吗？**

不能用，目前产业界没有应用

## **长短期记忆不仅仅是存储**

大模型的记忆问题大模型只有"7秒记忆"，<font color="#ffc000">模型窗口长度就是他的记忆空间，给的prompt就是他的全部记忆</font>

很多团队想做“记忆”功能，但这不是随手一加的开关，得有方案设计。要落地，需要同时考虑用户体验、隐私保护和系统架构。现在不少实现只是把对话记录简单归档，充其量是历史消息库，离真正有用的“记忆”还有差距
![[Pasted image 20251101223124.png]]

要把记忆做好，建议分层设计

- 用户层：记录个人偏好（如图表样式、写作语气）
- 团队层：沉淀常见问题、常用看板、运行手册
- 组织层：汇总知识库、规章制度和历史决策

个性化必须是透明的,用户应能清楚看到系统记录了什么，并可随时查看、修改或关闭相关记忆项

### **记忆的个性化**

记忆得让人看得见、改得了。搞 memory 系统，必须让用户知道"你记住了我什么"，而且得让用户自己能改。不然这玩意儿就不叫记忆，叫监控了。Agent 要稳定跑起来，记忆得发挥两个作用

1. 根据用户习惯做定制 —— 了解他的写作风格、常用格式、专业领域，给他量身定做
2. 主动出击，别光等着聊天 —— 根据发生的事件和数据，主动提供帮助，而不是只会被动回应

很多商业产品刚上线时都有个难题，比如 Uber 开发的对话工具，一开始根本不知道用户会问什么问题

他们根据用户过去的查询记录，建议记忆，推荐相关话题作为开场。但是个性化的帮助可能会越界侵犯隐私。比如让ChatGPT推荐一些适合全家看的电影，AI却根据家庭成员的名字给出影片的推荐，直接侵犯的用户的隐私[^1]

[^1]: 所以，用一个本地文件作为记忆，将用户输入的prompt和记忆文件作为上下文模型的输入，生成新的提示词？这样应该会更好。


### **记忆的冲突和平衡**

记忆功能能够改善用户体验并提升 Agent 的交互流畅度，但过度个性化很容易迅速触及隐私问题，如果不加以严格的范围限制，共享内存可能会突破访问控制

这里缺失了一个关键的基础要素：一种安全、可跨应用使用、由用户控制的便携式内存层。它不会被服务的提供商锁定。目前还没有人成功实现这一点。互联网公司，比如百度，360等公司产品，拼命记忆用户信息，推荐贷款，医疗等广告信息，给使用者带来很多感官不适


## **不要迷信单模型包打天下**

在生产环境中，不能所有的推理都用最好的模型GPT5,成本和模型会直接炸掉，表现也未必好。团队越来越多地基于以下内容运行模型路由逻辑：任务复杂度，推理延时要求，成本，数据本地，查询类型(例如，摘要 vs 语义搜索 vs 结构化问答)

真正合理的系统，一定是：快速反应的轻模型做分类和前处理、重模型做主任务、补一个模型做验证或追问。

一个 agent 后面绑定的一定是一个 LLM 团队。经常使用模型分层和路由如下：

1、对于琐碎的查询 → 本地模型(无网络调用)

2、对于结构化查询 → 调用DSL → SQL转换器

3、对于复杂分析 → 调用OpenAI / Anthropic / Gemini

4、回退或验证 → 双模型冗余(评判者+响应者)

简单问题由小型且快速的模型处理，而复杂的推理任务则交由先进的大模型完成。其核心理念是：通过持续追踪不同模型对各类查询的处理效果，模型的选择过程本身也可以随着时间不断学习和优化。

## **Agent ≠ Chatbot**

如果还在用聊天当所有用户交互的方式，那agent 最多是个语音助理。

真的 agent 应该是：先用语言调度任务，然后在页面上看到结构化结果，还能继续点选、调整、组合下一步。这部分很多公司现在在尝试了，交互上比之前全部自然语言高效了太多。

## **生成式 AI 领域的未来方向**

在讨论中提到了一些还没有被深入挖掘的方向，但其实它们是真正有待产品化的核心组件：

### **可追溯/可控/可信**

可追溯/可控/可信，是企业愿意用 Agent 的底线，很多人只想着怎么让 agent 能回答，但企业更关心：这句话是从哪里来的？有没有越权？出了错我怎么追责？

### **上下文可观测性**

哪些输入能够持续提升输出质量？什么样的上下文容易引发模型幻觉？你该如何像测试模型提示词那样来测试上下文？

目前，大多数团队都处于盲目前行的状态，缺乏系统性的方法来评估哪些上下文真正提升了模型性能，哪些反而造成了负面影响。

### **记忆系统**

记忆的是否可以随用户携带（而非依附于应用），具备安全性和可移植性，并支持用户按需选择组织、团队或个人状态的层级？

- • 用户不需要在每个新工具中重新建立上下文
- • 隐私和安全由用户掌控，而不是被服务提供商限制

这是整个技术栈中最关键的缺失环节。

### **面向特定领域的[DSL语言](https://zhida.zhihu.com/search?content_id=265778318&content_type=Article&match_order=1&q=DSL%E8%AF%AD%E8%A8%80&zhida_source=entity)**

大多数企业用户的需求都是结构化且重复的。与其费力地将自然语言解析成容易出错的 SQL，为何不直接设计更高层次、具备约束安全性且更可靠的专用语言（DSL）呢？

有团队建议，不应该局限于文本转 SQL，而是应该构建一个语义化的企业业务逻辑层，例如“显示第四季度收入”直接对应到一个经过验证的计算方法，而不是直接生成原始 SQL。

