# 图像处理中的注意力机制笔记

​		根据注意力机制应用于域的不同，即注意力权重施加的方式和位置不同，注意力机制分为空间域、通道域和混合域。![[Pasted image 20250619212530.png]]

## 1. 空间域注意力方法

​		对于卷积神经网络，CNN每一层都会输出一个C x H x W的特征图，C就是通道，同时也代表卷积核的数量，亦为特征的数量，H 和W就是原始图片经过压缩后的图的高度和宽度，而空间注意力就是对于所有的通道，在二维平面上，对H x W尺寸的特征图学习到一个权重矩阵，对应每个像素都会学习到一个权重。而这些权重代表的就是某个空间位置信息的重要程度 ，将该空间注意力矩阵附加在原来的特征图上，增大有用的特征，弱化无用特征，从而起到特征筛选和增强的效果。代表的Self-Attention、Non-local Attention以及Spatial Transformer等。

### 1.1 自注意力：Self-Attention

​	自注意力的结构下图所示，它是从NLP中借鉴过来的思想，因此仍然保留了Query, Key和Value等名称。对应图中自上而下分的三个分支，计算时通常分为三步：

(1) 第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；

(2) 第二步一般是使用一个softmax函数对这些权重进行归一化，转换为注意力；

(3) 第三步将权重和相应的键值value进行加权求和得到最后的attention。

<img src="C:\Users\Administrator\Desktop\笔记\image\Self-Attention.png" alt="Self-Attention" style="zoom: 50%;" />

​		自注意力是基于特征图本身的关注而提取的注意力。对于卷积而言，卷积核的设置限制了感受野的大小，导致网络往往需要多层的堆叠才能关注到整个特征图。而自注意的优势就是它的关注是全局的，它能通过简单的查询与赋值就能获取到特征图的全局空间信息。

### 1.2 非局部注意力Non-local Attention

​		Non-local Attention是研究self-attention在CV领域应用非常重要的文章。主要思想也很简单，CNN中的卷积单元每次只关注邻域kernel size 的区域，就算后期感受野越来越大，终究还是局部区域的运算，这样就忽略了全局其他片区（比如很远的像素）对当前区域的贡献。所以Non-local blocks 要做的是，捕获这种long-range 关系：对于2D图像，就是图像中任何像素对当前像素的关系权值；对于3D视频，就是所有帧中的所有像素，对当前帧的像素的关系权值。

<img src="C:\Users\Administrator\Desktop\笔记\image\Non-local-Attention.png" alt="Non-local-Attention" style="zoom:50%;" />

​		论文中有谈及多种实现方式，在这里简单说说在深度学习框架中最好实现的Matmul 方式，如上所示：

(1) 首先对输入的feature map X 进行线性映射（说白了就是1*1*1 卷积，来压缩通道数），然后得到θ，Φ，g特征；

(2) 通过reshape操作，强行合并上述的三个特征除通道数外的维度，然后对θ，Φ进行矩阵点乘操作，得到类似协方差矩阵的东西（这个过程很重要，计算出特征中的自相关性，即得到每帧中每个像素对其他所有帧所有像素的关系）；

(3) 然后对自相关特征以列或以行（具体看矩阵g 的形式而定） 进行Softmax 操作，得到0~1的权重，这里就是我们需要的Self-attention 系数；

(4) 最后将attention系数，对应乘回特征矩阵g 中，然后再与原输入的特征图残差一下，获得non-local block的输出。

## 2. 通道域注意力方法

​		通道域注意力类似于给每个通道上的特征图都施加一个权重，来代表该通道与关键信息的相关度的话，这个权重越大，则表示相关度越高。在神经网络中，越高的维度特征图尺寸越小，通道数越多，通道就代表了整个图像的特征信息。如此多的通道信息，对于神经网络来说，要甄别筛选有用的通道信息是很难的，这时如果用一个通道注意力告诉该网络哪些是重要的，往往能起到很好的效果，这时CV领域做通道注意力往往比空间好的一个原因。代表的是SENet、SKNet、ECANet等。

### 2.1 SENet

#### 2.1.1介绍

​		最后一届 ImageNet 2017 竞赛 Image Classification 任务的冠军，并被邀请在 CVPR 2017 的 workshop（Beyond ImageNet）中给出算法介绍。

​		SE注意力在CV领域有很强的普适性。

#### 2.1.2网络结构

<img src="C:\Users\Administrator\Desktop\笔记\image\SENet.png" alt="SENet" style="zoom: 50%;" />

​		上图是SENet的模型结构，该注意力机制主要分为三个部分：挤压(squeeze)，激励(excitation)，以及注意(scale )

​		给定一个输入 x，其特征通道数为 c1，通过一系列卷积等一般变换后得到一个特征通道数为 c2 的特征。与传统的 CNN 不一样的是，接下来我们通过三个操作来重标定前面得到的特征。

​		首先是 Squeeze 操作，从空间维度来进行特征压缩，使用全局平均池化将每个二维的特征通道变成一个实数，这个实数某种程度上具有全局的感受野，并且输出的维度和输入的特征通道数相匹配。最后h * w * c的特征变成了一个1 * 1 * c的特征，它表示在特征通道上响应的全域性分布，而且使得靠近输入的层也可以获得全局的感受野。

​		其次是 Excitation 操作，它是一个类似于循环神经网络中门的机制。通过引入 w 参数来为每个特征通道生成权重，其中 w 就是一个多层感知器，是可学习的，中间经过一个降维，减少参数量。并通过一个 Sigmoid 函数获得 0~1 之间归一化的权重，完成显式地建模特征通道间的相关性。

​		最后是一个 Scale 的操作，将 Excitation 的输出的权重看做是经过选择后的每个特征通道的重要性，通过通道宽度相乘加权到先前的特征上，完成在通道维度上的对原始特征的重标定。

<img src="C:\Users\Administrator\Desktop\笔记\image\SENet1.png" alt="SENet1" style="zoom:50%;" />

​		上左图是将 SE 模块嵌入到 Inception 结构的一个示例。方框旁边的维度信息代表该层的输出。

​		这里我们使用 global average pooling 作为 Squeeze 操作。紧接着两个 Fully Connected 层组成一个 Bottleneck 结构去建模通道间的相关性，并输出和输入特征同样数目的权重。我们首先将特征维度降低到输入的 1/16，然后经过 Rule 激活后再通过一个 Fully Connected 层升回到原来的维度。这样做比直接用一个 Fully Connected 层的好处在于：

1. 具有更多的非线性，可以更好地拟合通道间复杂的相关性；
2. 极大地减少了参数量和计算量。然后通过一个 Sigmoid 的门获得 0~1 之间归一化的权重，最后通过一个 Scale 的操作来将归一化后的权重加权到每个通道的特征上。

​		除此之外，SE 模块还可以嵌入到含有 skip-connections 的模块中。上右图是将 SE 嵌入到 ResNet 模块中的一个例子，操作过程基本和 SE-Inception 一样，只不过是在 Addition 前对分支上 Residual 的特征进行了特征重标定。如果对 Addition 后主支上的特征进行重标定，由于在主干上存在 0~1 的 scale 操作，在网络较深 BP 优化时就会在靠近输入层容易出现梯度消散的情况，导致模型难以优化。

#### 2.1.3 模型和计算复杂度

​		从上面的介绍中可以发现，SENet 构造非常简单，而且很容易被部署，不需要引入新的函数或者层。除此之外，它还在模型和计算复杂度上具有良好的特性。拿 ResNet-50 和 SE-ResNet-50 对比举例来说，SE-ResNet-50 相对于 ResNet-50 有着 10% 模型参数的增长。额外的模型参数都存在于 Bottleneck 设计的两个 Fully Connected 中，由于 ResNet 结构中最后一个 stage 的特征通道数目为 2048，导致模型参数有着较大的增长，实验发现移除掉最后一个 stage 中 3 个 build block 上的 SE 设定，可以将 10% 参数量的增长减少到 2%。此时模型的精度几乎无损失。

​		另外，由于在现有的 GPU 实现中，都没有对 global pooling 和较小计算量的 Fully Connected 进行优化，这导致了在 GPU 上的运行时间 SE-ResNet-50 相对于 ResNet-50 有着约 10% 的增长。尽管如此，其理论增长的额外计算量仅仅不到 1%，这与其在 CPU 运行时间上的增长相匹配（~2%）。可以看出，在现有网络架构中嵌入 SE 模块而导致额外的参数和计算量的增长微乎其微。

### 2.2 SKNet

​		SKNet是基于SENet的改进，他的思路是在提高精度。而很多网络使用了各种Trick来降低计算量，比如SENet多层感知机间添加了降维。SKNet就是想如果不牺牲那么多计算量，能否精度提高一些呢？因此它设置了一组动态卷积选择来实现精度提升。

<img src="C:\Users\Administrator\Desktop\笔记\image\SKNet.png" alt="SKNet" style="zoom:50%;" />

​		上图所示是SKNet的基本结构。主要创新点是设置了一组动态选择的卷积，分为三个部分操作Split、Fuse、Select。

（1）Split：对输入向量X进行不同卷积核大小的完整卷积操作（组卷积），特别地，为了进一步提升效率，将5x5的传统卷积替代为dilation=2，卷积核为3x3的空洞卷积；

（2）Fuse：类似SE模块的处理，两个feature map相加后，进行全局平均池化操作，全连接先降维再升维的为两层全连接层，输出的两个注意力系数向量a和b，其中a+b=1；

（3）Select： Select操作对应于SE模块中的Scale。Select使用a和b两个权重矩阵对之前的两个feature map进行加权操作，它们之间有一个类似于特征挑选的操作。

## 3.混合域注意力方法

​		空间与通道结合的混合域注意力机制。思想：通道和空间对网络都有提升作用，那么它们间的有效结合必定会给网络带来更大的促进作用。根据DL任务的不同，它们结合方式也存在区别，有代表性的是CBAM、DANet、CCNet、Residual Attention等

### 3.1 CBAM

​		基于SENet的改进，论文中把 channel-wise attention 看成是教网络 Look ‘what’；而spatial attention 看成是教网络 Look ‘where’，所以它比 SE Module 的主要优势就多了后者。

<img src="C:\Users\Administrator\Desktop\笔记\image\CBAM.png" alt="CBAM" style="zoom:50%;" />

​		上图所示是CBAM的基本结构，前面是一个使用SENet的通道注意力模块，后面的空间注意力模块设计也参考了SENet，它将全局平均池化用在了通道上，因此作用后就得到了一个二维的空间注意力系数矩阵。值得注意的是，CBAM在空间与通道上同时做全局平均和全局最大的混合pooling，能够提取到更多的有效信息。

### 3.2 DANet

​		DANet来自于CVPR 2019的文章Dual Attention Network for Scene Segmentation，注意思想也是参考了上述提到的CBAM 和Non-local 的融合变形。具体来说就是，结构框架使用的是CBAM，具体方法使用的是self-attention。

![DANet](C:\Users\Administrator\Desktop\笔记\image\DANet.png)

​		上图所示是DANet注意力模块的基本结构，主要包括Position Attention Module 和 Channel Attention Module。两个模块使用的方法都是self-attention，只是作用的位置不同，一个是空间域的self-attention，一个是通道域的self-attention。这样做的好处是：在CBAM 分别进行空间和通道self-attention的思想上，直接使用了non-local 的自相关矩阵Matmul 的形式进行运算，避免了CBAM 手工设计pooling，多层感知器等复杂操作。同时，把Self-attention的思想用在图像分割，可通过long-range上下文关系更好地做到精准分割。