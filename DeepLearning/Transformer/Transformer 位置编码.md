# Transformer 位置编码



## 1. Transformer在NLP中的位置编码

**Positional Encoding(PE)**第一次出现在谷歌的Attention is all you need中。

### 1.1 为什么需要位置编码

1. 句子中的单词需要有顺序。
2. Transformer模型抛弃了RNN和CNN这种天然还包含位置信息的网络结构，所以失去了词序信息。

### 1.2 what is PE?

位置编码（Positional Encoding）是一种用词的位置信息对序列中的每个词进行二次表示的方法。正如前文所述，Transformer模型本身不具备像RNN那样的学习词序信息的能力，需要主动将词序信息喂给模型。那么，模型原先的输入是不含词序信息的词向量，位置编码需要将词序信息和词向量结合起来形成一种新的表示输入给模型，这样模型就具备了学习词序信息的能力。

### 1.3 How to implement PE?

一种好的位置编码方案需要满足以下几条要求：

- 它能为每个时间步输出一个**独一无二**的编码；
- 不同长度的句子之间，任何两个**时间步之间的距离应该保持一致**；
- 模型应该能毫不费力地**泛化**到更长的句子且它的**值应该是有界的**；
- 它必须是**确定性的**。

#### Transformer中的位置编码

Transformer的作者们提出了一个简单但非常创新的位置编码方法，能够满足上述所有的要求。首先，这种编码不是单一的一个数值，而是包含句子中特定位置信息的![[公式]](https://www.zhihu.com/equation?tex=d)维向量（非常像词向量)。第二，这种编码没有整合进模型，而是用这个向量让每个词具有它在句子中的位置的信息。换句话说，通过注入词的顺序信息来增强模型输入。

