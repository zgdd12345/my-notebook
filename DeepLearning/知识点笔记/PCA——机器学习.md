# PCA——机器学习

### 0.概述

PCA（Principal Component Analysis） 是一种常见的数据分析方式，常用于高维数据的降维，可用于提取数据的主要特征分量。

PCA 的数学推导可以从最大可分型和最近重构性两方面进行，前者的优化条件为划分后方差最大，后者的优化条件为点到划分平面距离最小，这里我将从最大可分性的角度进行证明。## 1. 向量表示与基变换

线性代数的基本知识。

### 1.1 内积

两个向量的 A 和 B 内积我们知道形式是这样的：

<img src="C:\Users\Administrator\Desktop\笔记\机器学习\image\内积.jpg" alt="内积" style="zoom: 50%;" />

内积运算将两个向量映射为实数，其计算方式非常容易理解，但我们无法看出其物理含义。接下来我们从几何角度来分析，为了简单起见，我们假设 A 和 B 均为二维向量，则：

<img src="C:\Users\Administrator\Desktop\笔记\机器学习\image\内积2.jpg" alt="内积2" style="zoom:50%;" />

其几何表示见下图：

<img src="C:\Users\Administrator\Desktop\笔记\机器学习\image\内积3.jpg" alt="内积3" style="zoom:50%;" />

我们看出 A 与 B 的内积等于 A 到 B 的投影长度乘以 B 的模。

如果假设 B 的模为 1，即让 ![[公式]](https://www.zhihu.com/equation?tex=%7CB%7C%3D1) ，那么就变成了：

![[公式]](https://www.zhihu.com/equation?tex=A%5Ccdot+B%3D%7CA%7Ccos%28a%29+%5C%5C)

也就是说，**A 与 B 的内积值等于 A 向 B 所在直线投影的标量大小。**

这就是内积的一种几何解释，也是我们得到的第一个重要结论。在后面的推导中，将反复使用这个结论。

### 1.2 基

在我们常说的坐标系中，向量 (3,2) 其实隐式引入了一个定义：以 x 轴和 y 轴上正方向长度为 1 的向量为标准。向量 (3,2) 实际是说在 x 轴投影为 3 而 y 轴的投影为 2。**注意投影是一个标量，所以可以为负。**

所以，对于向量 (3, 2) 来说，如果我们想求它在 ![[公式]](https://www.zhihu.com/equation?tex=%281%2C0%29%2C%280%2C1%29) 这组基下的坐标的话，分别内积即可。当然，内积完了还是 (3, 2)。

所以，我们大致可以得到一个结论，我们**要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了**。为了方便求坐标，我们希望这组基向量模长为 1。因为向量的内积运算，当模长为 1 时，内积可以直接表示投影。然后还需要这组基是线性无关的，我们一般用正交基，非正交的基也是可以的，不过正交基有较好的性质。



