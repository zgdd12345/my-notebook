# 转置卷积

​		转置卷积又叫反卷积、逆卷积。TensorFlow，Pytorch，Keras中的函数名都是conv_transpose。

### 1.卷积在计算机中的计算过程

​		在计算机中计算的时候，并不是像这样一个位置一个位置的进行滑动计算，因为这样的效率太低了。计算机会将卷积核转换成等效的矩阵，将输入转换为向量。通过输入向量和卷积核矩阵的相乘获得输出向量。输出的向量经过整形便可得到我们的二维输出特征。具体的操作如下图所示。由于我们的3x3卷积核要在输入上不同的位置卷积4次，所以通过补零的方法将卷积核分别置于一个4x4矩阵的四个角落。这样我们的输入可以直接和这四个4x4的矩阵进行卷积，而舍去了滑动这一操作步骤。

<img src="C:\Users\Administrator\Desktop\笔记\python及相应工具笔记\source\转置卷积1.jpg" alt="转置卷积1" style="zoom: 50%;" />

​		进一步的，我们将输入拉成长向量，四个4x4卷积核也拉成长向量并进行拼接，如下图。

<img src="C:\Users\Administrator\Desktop\笔记\python及相应工具笔记\source\转置卷积2.jpg" alt="转置卷积2" style="zoom:50%;" />

我们记向量化的图像为I ,向量化的卷积矩阵为C , 输出特征向量为 O 
则有：
$$
I^T*C=O^T
$$
如下图所示。
<img src="C:\Users\Administrator\Desktop\笔记\python及相应工具笔记\source\转置卷积3.jpg" alt="转置卷积3" style="zoom:50%;" />

​		我们将一个1x16的行向量乘以16x4的矩阵，得到了1x4的行向量。**那么反过来将一个1x4的向量乘以一个4x16的矩阵是不是就能得到一个1x16的行向量呢？** 没错，这便是**转置卷积**的思想。

### 2.转置卷积

​		一般的卷积操作（只考虑padding, stride=1的情况），都将输入的数据越卷越小。根据卷积核大小的不同，和步长的不同，输出的尺寸变化也很大。但是有的时候我们需要输入一个小的特征，输出更大尺寸的特征该怎么办呢？比如图像语义分割中往往要求最终输出的特征尺寸和原始输入尺寸相同，但在网络卷积核池化的过程中特征图的尺寸却逐渐变小。在这里转置卷积便能派上了用场。在数学上，转置卷积的操作也非常简单，把正常卷积的操作反过来即可。
对应上面公式，我们有转置卷积的公式：
$$
O^T*C^T=I^T
$$
![转置卷积4](C:\Users\Administrator\Desktop\笔记\python及相应工具笔记\source\转置卷积4.jpg)

​		这里需要注意的是这两个操作并**不是可逆**的，对于同一个卷积核，经过转置卷积操作之后并不能恢复到原始的数值，保留的只有原始的形状。
​		所以转置卷积的名字就由此而来，而不是“反卷积”或者是“逆卷积”，不好的名称容易给人以误解。