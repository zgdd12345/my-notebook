#LLM 
## 摘要



---
### 文献综述

近年来，随着大规模语言预训练模型的发展，自然语言处理领域的主流范式已经转向基于大规模通用数据的预训练与面向特定任务或领域的适应。然而，随着模型参数量的不断激增，传统的微调（fine-tuning）方法变得不再高效和可扩展。

为应对此问题，Hu等人于2021年提出了Low-Rank Adaptation (LoRA)方法，通过冻结预训练模型参数，并在Transformer结构的各层引入可训练的低秩分解矩阵，显著减少了下游任务的可训练参数量。

LoRA在GPT-3上的实验结果显示，其可将可训练参数数量减少到全量微调的万分之一，且计算硬件需求降至三分之一。此外，LoRA方法在模型质量上与全参数微调表现相当甚至更优，并且具备更高的训练吞吐量与无额外推理时延。

该研究还对语言模型适应中的秩不足现象进行了实证分析，为LoRA效能提供理论解释。

尽管LoRA大幅降低了硬件资源消耗和参数存储需求，其实际在多样化任务和更广泛模型架构下的通用性及实际应用限制仍需进一步探讨。总体来看，LoRA为高效适应大模型提供了创新思路，在降低训练和部署成本方面具有重要意义。

---
### 文献矩阵

|文献|研究目标|方法|数据集/模型|主要发现|局限性|
|---|---|---|---|---|---|
|Hu等人2021[1]|高效适应大规模语言模型，降低微调成本|冻结预训练模型，仅在各层插入低秩分解可训练矩阵，即LoRA|GPT-2、GPT-3|可训练参数显著减少（达万分之一），硬件需求降低3倍，模型效果与全量微调相当甚至更优，无额外推理时延|多样化任务下的通用性有待进一步验证|
|**共性特征**|高效参数适应与微调|减少参数规模与计算成本的方法|基于Transformer的大型模型|关注大模型低成本适应和实际部署落地|关注实际应用中的资源与效率问题|
|**差异性特征**|按秩矩阵分解为创新点|不同于传统微调，采用低秩分解|主要实验于GPT-2/3|兼顾参数高效性与模型性能|需验证对其它模型结构与更多任务的适应性|
