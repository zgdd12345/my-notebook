## 1. 文献数据下载

###  arxive下载
已完成
### 教材
一本

## 2. PDF文档解析

[解析工具MinerU](https://github.com/opendatalab/MinerU/blob/master/README_zh-CN.md)
### 安装 MinerU

#### 使用pip或uv安装MinerU

```shell
pip install --upgrade pip -i https://mirrors.aliyun.com/pypi/simple
pip install uv -i https://mirrors.aliyun.com/pypi/simple
uv pip install -U "mineru[core]" -i https://mirrors.aliyun.com/pypi/simple 
```

### 使用 MinerU
最简单的命令行调用方式:
``` shell

mineru -p <input_path> -o <output_path>

# e.t.
mineru -p “../papers/quant-ph_9902053v1.A_better_lower_bound_for_quantum_algorithms_searching_an_ordered_list.pdf” -o "./" --source local -d cuda

# batch process
mineru -p “../papers” -o "./pdfresults" --source local -d cuda -l en
```

您可以通过命令行、API、WebUI等多种方式使用MinerU进行PDF解析，具体使用方法请参考[使用指南](https://opendatalab.github.io/MinerU/zh/usage/)。

配置模型源
```
export MINERU_MODEL_SOURCE=modelscope
export MINERU_DEVICE_MODE=cuda
```

参数：
``` shell
mineru --help
Usage: mineru [OPTIONS]

Options:
  -v, --version                   显示版本并退出
  -p, --path PATH                 输入文件路径或目录（必填）
  -o, --output PATH               输出目录（必填）
  -m, --method [auto|txt|ocr]     解析方法：auto（默认）、txt、ocr（仅用于 pipeline 后端）
  -b, --backend [pipeline|vlm-transformers|vlm-sglang-engine|vlm-sglang-client]
                                  解析后端（默认为 pipeline）
  -l, --lang [ch|ch_server|ch_lite|en|korean|japan|chinese_cht|ta|te|ka|th|el|latin|arabic|east_slavic|cyrillic|devanagari]
                                  指定文档语言（可提升 OCR 准确率，仅用于 pipeline 后端）
  -u, --url TEXT                  当使用 sglang-client 时，需指定服务地址
  -s, --start INTEGER             开始解析的页码（从 0 开始）
  -e, --end INTEGER               结束解析的页码（从 0 开始）
  -f, --formula BOOLEAN           是否启用公式解析（默认开启）
  -t, --table BOOLEAN             是否启用表格解析（默认开启）
  -d, --device TEXT               推理设备（如 cpu/cuda/cuda:0/npu/mps，仅 pipeline 后端）
  --vram INTEGER                  单进程最大 GPU 显存占用(GB)（仅 pipeline 后端）
  --source [huggingface|modelscope|local]
                                  模型来源，默认 huggingface
  --help                          显示帮助信息
```

批量处理的shell脚本

``` Shell
#!/bin/bash
INPUT_DIR="./papers"
OUTPUT_DIR="./pdfresults"

mkdir -p "$OUTPUT_DIR"

# 遍历所有 pdf 文件
for pdf in $INPUT_DIR/*.pdf; do
    # 确保文件真实存在
    [ -f "$pdf" ] || continue

    echo "正在解析: $pdf"
    mineru -p "$pdf" -o "$OUTPUT_DIR" --source local -d cuda

    if [ $? -ne 0 ]; then
        echo "❌ 解析失败: $pdf" >> "$OUTPUT_DIR/failed.log"
    else
        echo "✅ 解析成功: $pdf" >> "$OUTPUT_DIR/success.log"
    fi
done

echo "全部处理完成！结果在 $OUTPUT_DIR"

```


## 3. Markdown 文档内容解析与保留策略

在准备约 2000 篇英文 Markdown 格式学术论文的数据集时，首先要确保关键内容的保留和正确解析。具体策略包括：

- **LaTeX 公式保留：** 不要简单去除 `$...$` 或 `$$...$$` 中的内容。相反，应保留这些公式原样，以供模型学习数学符号和推理能力。例如，OpenWebMath 数据集在提取文本时特别强调 “维护 LaTeX 方程的完整性”。与一些清洗策略相反（有些管道会用正则表达式直接移除 `$...$` 内容），我们需要保留公式用于下游任务。
- **图表及其说明：** 保留论文中的图表描述（Caption）。Markdown 中通常通过 `![说明文本](图片路径)` 表示图像，我们可以**去除图片路径但保留说明文本**。例如，将`!\[Caption\](image.png)`转换为“Figure: Caption”以保留图例描述。这样模型可以学习图表的语义信息。此前有人选择直接移除图片链接，但在本任务中，我们要保留 **图表说明** 供模型理解和微调使用。
- **代码块：** Markdown 中的代码块（用三个反引号 \`\`\` 包裹）应完整保留。不要去除或修改代码内部内容。通过保留格式，模型在预训练时能学习代码模式，并在微调时支持代码相关任务。确保在转换过程中保留反引号标记，避免将代码块转换为普通文本格式。
- **结构化标题：** 学术论文通常有层次分明的章节标题（如 `# Introduction`, `## Methodology` 等）。建议保留这些标题标记或替换为明确的段落标签，以反映文档结构。这有助于模型学习段落组织，提高内容连贯性。可以直接保留 Markdown 的井号 `#` 标题语法，或在纯文本输出中用特殊标记（例如 `[Section] 引言:`）表示。采用Pandoc等工具转换时也应注意**保留结构化信息**。
- **其他Markdown元素：** 可以移除对下游任务无用的纯格式标记，但保留所有对内容理解有帮助的元素。例如，去掉多余的HTML标签、脚注标记等，但**保留列表项、表格文本**等结构。对于 Markdown 链接 `[文本](URL)`，可去掉 URL 部分但保留可读的链接文字。同样地，表格如果在转换中格式混乱，需要在文本中保持表格内容的连贯（例如用制表符或管道符号保留表格结构，或转为简洁的列表形式）。

以上策略可确保模型预训练语料中包含公式、图表说明、代码和标题结构，有利于后续多任务微调。

可完整保留PDF解析生成的markdown文档作为预训练数据

## 4. 预训练数据集的格式与生成

在完成内容保留和初步清洗后，需要将论文集合转换为适用于预训练的纯文本数据格式。通常做法是**采用 JSON Lines 格式**，每篇论文或每个文本段作为一条记录。例如，可以构造如下格式的输出：

`{ "text": "<论文文本>", "meta": { "section": "Introduction", "lang": "en", "has_formula": true, "source": "paper_123.md" } }`

其中 `text` 字段包含论文内容（纯文本形式，但保留 Markdown 结构元素如公式、代码块等），`meta` 包含元信息。需要注意：

- **纯文本内容：** 将Markdown解析为纯文本但**尽量保留原始格式**（如公式用 `$...$` 表示，代码仍用 ``` 包围，列表项以 `-` 表示，标题以 `#` 表示）。这样既保证格式信息不丢失，又提供连续的文本流供预训练模型学习。如果直接用Pandoc转换为TXT，要注意核对转换质量，确保知识完整迁移。
    
- **清洗与规范化：** 执行适度的清洗，使文本适合模型训练。例如，移除 Markdown 语法中的URL、图像路径等不必要内容，只保留对理解有用的文本[blog.csdn.net](https://blog.csdn.net/qq_55773484/article/details/142824648#:~:text=,%E5%90%88%E5%B9%B6%E5%A4%9A%E4%BD%99%E7%A9%BA%E7%99%BD%E4%B8%BA%E4%B8%80%E4%B8%AA%E7%A9%BA%E6%A0%BC)。合并多余空白、标准化换行。同时避免过度清洗导致信息缺失。
    
- **段落与分块：** 可以选择每篇论文作为单独的文本条目，也可以按章节/段落切分长文档。由于大模型可以处理长上下文，完整论文作为一个样本有助于模型学习长文依赖。但同时也可以**按语义边界拆分**，例如基于章节或自然段落，将长文档拆成多个片段[blog.csdn.net](https://blog.csdn.net/qq_55773484/article/details/142824648#:~:text=%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%98%B6%E6%AE%B5%20%E6%B5%81%E7%A8%8B%E7%9B%AE%E5%89%8D%E7%A1%AE%E5%AE%9A%E5%A4%A7%E8%87%B4%E6%98%AF%3A%20%E5%85%88%E6%96%87%E6%9C%AC%E6%8F%90%E5%8F%96%EF%BC%8C%20%E7%84%B6%E5%90%8E%E5%AF%B9%E5%86%85%E5%AE%B9%E8%BF%9B%E8%A1%8C%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%87%E5%88%86%EF%BC%8C%E5%AF%B9%E6%96%87%E6%9C%AC%E4%B8%AD%E7%9A%84%E5%86%85%E5%AE%B9%E8%BF%9B%E8%A1%8C%E5%88%86%E5%9D%97%EF%BC%8C%E8%BF%99%E4%B8%80%E6%AE%B5%E6%8C%89%E7%85%A7%E8%87%AA%E7%84%B6%E8%AF%AD%E4%B9%89%E7%9A%84%E8%BE%B9%E7%95%8C%E8%BF%9B%E8%A1%8C%E3%80%82%E7%84%B6%E5%90%8E%E5%AF%B9%E6%96%87%E6%9C%AC%E5%86%85%E5%AE%B9%E8%BF%9B%E8%A1%8C%E9%87%8D%20%E5%8F%A0%E8%AE%BE%E7%BD%AE%EF%BC%8C%E6%8F%90%E5%8D%87%E8%AF%AD%E4%B9%89%E7%9A%84%E8%BF%9E%E8%B4%AF%E6%80%A7%EF%BC%8C%E2%80%9C%E5%86%8D%E5%B0%B1%E6%98%AF%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%EF%BC%8C%E5%8E%BB%E9%99%A4%E5%A4%9A%E4%BD%99%E5%86%85%E5%AE%B9%E5%92%8C%E6%97%A0%E6%95%88%E4%BF%A1%E6%81%AF%EF%BC%8C%E6%9C%80%E5%90%8E%E8%BE%93%E5%87%BA%E4%B8%8D%E5%90%8C,%E3%80%82%E7%84%B6%E5%90%8E%E5%AF%B9%E4%BA%8E%E8%A1%A8%E6%A0%BC%E7%B1%BB%E7%9A%84%E4%B8%9C%E8%A5%BF%EF%BC%8C%E6%AF%94%E5%A6%82pdf%E4%B8%AD%E6%9C%AC%E8%BA%AB%E5%B0%B1%E5%AD%98%E5%9C%A8%E7%9A%84%E5%9B%BE%E8%A1%A8%EF%BC%8C%E5%9C%A8%20%E8%BD%AC%E6%8D%A2%E6%88%90%20markdown%20%E7%AD%89%E5%BD%A2%E5%BC%8F%E7%9A%84%E6%97%B6%E5%80%99%E4%BC%9A%E6%9C%89%E4%B8%80%E7%82%B9%E9%97%AE%E9%A2%98%EF%BC%8C%E5%B0%B1%E6%98%AF%E6%9C%89%E4%BA%9B%E4%BC%9A%E4%B8%80%E4%B8%AA%E8%A1%A8%E6%A0%BC%E8%A2%AB%E5%88%87%E5%89%B2%E6%88%90%E4%B8%A4%E4%B8%AA%E8%BF%99%E7%A7%8D%EF%BC%8C%E8%BF%98%E6%9C%89%E5%B0%B1%E6%98%AF%E8%BE%B9%E7%95%8C%E4%B8%8D%E5%A4%9F%E6%B8%85%E6%99%B0%EF%BC%8C%E8%BF%99%E4%BA%9B%E5%8F%AF%E8%83%BD%E9%9C%80%E8%A6%81%E4%BA%BA%E5%B7%A5%E6%A0%87%E6%B3%A8%E7%9A%84%E5%8A%9E%E6%B3%95%E6%9D%A5%E6%8F%90%E9%AB%98%E7%B2%BE%E7%A1%AE%E5%BA%A6%20%E6%9C%80%E6%96%B0%E5%8F%91%E5%B8%83)。拆分时确保段落连贯，不打断句子或公式。每个片段都存为一条记录，并在元数据中注明所属章节。这样既保留逻辑结构，又增加训练样本数。
    
- **样本示例：** 例如，对于一篇包含公式和图的论文，其预训练文本可以如下（简化）：
    
    `# Introduction   This paper proposes a new model for ...    ## Methodology   The method is based on the equation $E = mc^2$ where $E$ is energy...    ![Architecture diagram](fig1.png)   Figure: The architecture of our model consists of ...    ```python def algorithm(x):     # pseudocode for algorithm     ...`
    
    ## Results
    
    ...
    
    ``在JSON行中，这段文本会作为 `"text"` 值出现。注意，其中 `$E = mc^2$` 格式的公式、以 `Figure:` 引入的图例说明，以及 ```python``` 代码块均被保留下来。``
    
- **语言与编码：** 确保所有文本编码统一为UTF-8，并标注语言（本例全部为英文 `"lang": "en"`）。如果数据集中存在非英文内容，可用语言检测工具过滤或标记。对数学符号统一处理，比如确保使用一致的LaTeX表示而非混杂的Unicode符号。
    
- **去重与一致性：** 在聚合预训练语料时，要**去除重复或高度相似的内容**，以防模型过度学习某些样本。可以使用文本去重工具（如 **text-dedup** 库的 SimHash 算法）来识别近似重复的文档。同时，检查格式是否一致，例如所有公式定界符成对存在，代码块起止标记完整等。如发现表格在Markdown中被错误切分，可进行手工调整或采用 Pandas 重新整合表格再转回Markdown。
    

经过上述处理，将得到高质量的预训练文本数据集，每条样本都是结构清晰、信息完整的论文文本，为模型提供丰富的训练语料。







## 5.  指令微调数据集制作














## 6.  代码数据集制作









