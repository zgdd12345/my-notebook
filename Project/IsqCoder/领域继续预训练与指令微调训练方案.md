数据集制作见 [[制作预训练和微调数据集]]

## 模型训练方案

模型以Qwen-7B为baseline做实验

模型训练分为两个阶段：**领域继续预训练阶段**和**指令微调(SFT)阶段**。整个训练过程中采用参数高效微调技术（LoRA/QLoRA），以适配有限算力，同时减少对模型原有能力的干扰。下表给出了两个阶段训练中关键参数的建议配置：

|**配置项**|**领域继续预训练阶段**|**指令微调阶段**|
|---|---|---|
|基础模型|Qwen-7B (基础模型) 或 Qwen-7B-Chat (对话模型)|阶段一微调后的模型（融合领域知识后）|
|微调方法|LoRA 或 QLoRA（4-bit量化LoRA）[arxiv.org](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is)|LoRA 或 QLoRA（与阶段一相同方法，以保证效率）|
|LoRA rank|16（低秩适配矩阵秩，控制新增参数量）[arxiv.org](https://arxiv.org/html/2508.02932v1#:~:text=tuning,as%20shown%20in%20Figure%C2%A01)|16（可与阶段一相同，保持模型容量）|
|LoRA α|16（LoRA缩放系数，提高适配器权重影响）[medium.com](https://medium.com/@lyo.gavin/qlora-is-a-game-changer-open-sourcing-the-1st-chinese-33b-qola-model-2ccf6863ae5a#:~:text=%2A%20Learning%20rate%3A%201e,kept%20complete%20without%20being%20truncated)|16（与阶段一一致）|
|LoRA dropout|0 - 0.1（视过拟合情况，可选0或5%-10%随机失活）|0（指令数据量有限，尽量不引入额外随机性）|
|学习率 (LR)|1×10^-4 （预训练任务，相对较高学习率注入新知识）|5×10^-5 （指令微调，适度降低LR防止过拟合对话风格）|
|学习率调度|线性预热100-500步至基准LR，余下采用余弦退火降至0[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=Learning%20rate%20schedulers%20lower%20the,avoid%20overshooting%20the%20loss%20minima)|与阶段一类似（可适当缩短预热步数）|
|最大序列长度|2048 tokens（模型上下文长度，上限2048，训练时可采用）|1024 tokens（指令+回答通常较短，可设定为1024）|
|批大小 (batch)|有效批大小约16-32序列（单GPU可用较小batch配合梯度累积）[arxiv.org](https://arxiv.org/html/2508.02932v1#:~:text=Observation%20%234%3A%20LoRA%20Fine,We%20observe%20that%20LoRA)|有效批大小约16序列（单GPU场景下可适当减小）|
|训练轮次 (epochs)|2-3轮（覆盖约几十万步，保证每篇语料至少训练2遍以上）|1-2轮（视指令数据规模，防止过拟合[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=When%20I%20increased%20the%20number,a%20decline%20in%20model%20performance)）|
|优化器|AdamW (β1=0.9, β2=0.999，无或低权重衰减)|AdamW (与阶段一相同配置)|
|混合精度|FP16/BF16 （若非使用QLoRA，可开启混合精度节省显存）|FP16/BF16|
|梯度检查点|开启（减小长序列训练的显存占用）|开启（同左）|
|梯度累积|开启（如实际batch过小，通过梯度累积凑够有效批规模）|开启（同左）|

**表1：继续预训练阶段与指令微调阶段的训练参数建议。**

以上参数设置旨在在**保证效果**的前提下**提高训练效率**：

- **LoRA/QLoRA**：通过在每层添加少量可训练参数，显著减少训练开销，仅更新不到模型参数的1%即可达到全参数调优的效果[infohub.delltechnologies.com](https://infohub.delltechnologies.com/en-us/p/llama-2-efficient-fine-tuning-using-low-rank-adaptation-lora-on-single-gpu/#:~:text=LoRA%20is%20an%20efficient%20fine,optimizer%20states%20attached%20to%20them)[arxiv.org](https://arxiv.org/html/2508.02932v1#:~:text=tuning,as%20shown%20in%20Figure%C2%A01)。例如，在Qwen-7B上使用秩为64的LoRA时，仅需训练约3.4%的参数[arxiv.org](https://arxiv.org/html/2508.02932v1#:~:text=tuning,as%20shown%20in%20Figure%C2%A01)。QLoRA在此基础上将模型权重量化为4-bit，进一步将显存占用降低约33%，仅以约39%的额外训练时间为代价，性能几乎无损[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=backpropagation%2C%20QLoRA%20quantizes%20the%20pretrained,optimizers%20to%20handle%20memory%20spikes)[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=QLoRA%20with%204)。这使得65B模型在单张48GB卡上微调成为可能[arxiv.org](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is)。在本方案中，对8B模型采用LoRA/QLoRA，能轻松在单机单卡上完成训练任务。
    
- **学习率与调度**：继续预训练阶段采用略高的学习率，以较快融合新知识；指令微调阶段则适当降低学习率以精细调整生成风格。使用**预热+余弦退火**调度，有助于稳定收敛并避免过冲[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=Learning%20rate%20schedulers%20lower%20the,avoid%20overshooting%20the%20loss%20minima)。例如预热几百步至1e-4，然后缓慢降至接近0，既学习新知识又尽量不干扰原有知识分布。
    
- **批大小与轮次**：适度的有效批大小确保梯度估计稳定。《PLoRA》研究指出LoRA微调往往偏好**小批量**以取得更好效果[arxiv.org](https://arxiv.org/html/2508.02932v1#:~:text=Observation%20%234%3A%20LoRA%20Fine,We%20observe%20that%20LoRA)；因此我们在单卡显存有限时通过梯度累积实现一个中等批量（如16-32）。预训练语料重复训练2-3轮，以增大知识注入的深度；而指令数据集不宜轮次太多（1-2轮为宜），防止模型“背熟”答案影响泛化[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=When%20I%20increased%20the%20number,a%20decline%20in%20model%20performance)。
    
- **混合精度与梯度检查点**：这些技术在不影响最终效果的情况下节省显存，使长序列训练成为可能。特别地，如果使用QLoRA，需借助`bitsandbytes`库的4-bit优化器实现，梯度仍以16-bit计算。实验表明开启这些选项可以在不增加太多开销的情况下将7B模型LoRA微调的显存占用控制在15GB左右[infohub.delltechnologies.com](https://infohub.delltechnologies.com/en-us/p/llama-2-efficient-fine-tuning-using-low-rank-adaptation-lora-on-single-gpu/#:~:text=1,as%20illustrated%20in%20figure%202)[infohub.delltechnologies.com](https://infohub.delltechnologies.com/en-us/p/llama-2-efficient-fine-tuning-using-low-rank-adaptation-lora-on-single-gpu/#:~:text=Table%201,LoRA%20technique%20in%20our%20experiment)。