数据集制作见 [[制作预训练和微调数据集]]

## 模型训练方案

模型以Qwen-7B为baseline做实验
### 1 训练配置与参数
模型训练分为两个阶段：**领域继续预训练阶段**和**指令微调(SFT)阶段**。整个训练过程中采用参数高效微调技术（LoRA/QLoRA），以适配有限算力，同时减少对模型原有能力的干扰。下表给出了两个阶段训练中关键参数的建议配置：

|**配置项**|**领域继续预训练阶段**|**指令微调阶段**|
|---|---|---|
|基础模型|Qwen-7B (基础模型) 或 Qwen-7B-Chat (对话模型)|阶段一微调后的模型（融合领域知识后）|
|微调方法|LoRA 或 QLoRA（4-bit量化LoRA）[arxiv.org](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is)|LoRA 或 QLoRA（与阶段一相同方法，以保证效率）|
|LoRA rank|16（低秩适配矩阵秩，控制新增参数量）[arxiv.org](https://arxiv.org/html/2508.02932v1#:~:text=tuning,as%20shown%20in%20Figure%C2%A01)|16（可与阶段一相同，保持模型容量）|
|LoRA α|16（LoRA缩放系数，提高适配器权重影响）[medium.com](https://medium.com/@lyo.gavin/qlora-is-a-game-changer-open-sourcing-the-1st-chinese-33b-qola-model-2ccf6863ae5a#:~:text=%2A%20Learning%20rate%3A%201e,kept%20complete%20without%20being%20truncated)|16（与阶段一一致）|
|LoRA dropout|0 - 0.1（视过拟合情况，可选0或5%-10%随机失活）|0（指令数据量有限，尽量不引入额外随机性）|
|学习率 (LR)|1×10^-4 （预训练任务，相对较高学习率注入新知识）|5×10^-5 （指令微调，适度降低LR防止过拟合对话风格）|
|学习率调度|线性预热100-500步至基准LR，余下采用余弦退火降至0[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=Learning%20rate%20schedulers%20lower%20the,avoid%20overshooting%20the%20loss%20minima)|与阶段一类似（可适当缩短预热步数）|
|最大序列长度|2048 tokens（模型上下文长度，上限2048，训练时可采用）|1024 tokens（指令+回答通常较短，可设定为1024）|
|批大小 (batch)|有效批大小约16-32序列（单GPU可用较小batch配合梯度累积）[arxiv.org](https://arxiv.org/html/2508.02932v1#:~:text=Observation%20%234%3A%20LoRA%20Fine,We%20observe%20that%20LoRA)|有效批大小约16序列（单GPU场景下可适当减小）|
|训练轮次 (epochs)|2-3轮（覆盖约几十万步，保证每篇语料至少训练2遍以上）|1-2轮（视指令数据规模，防止过拟合[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=When%20I%20increased%20the%20number,a%20decline%20in%20model%20performance)）|
|优化器|AdamW (β1=0.9, β2=0.999，无或低权重衰减)|AdamW (与阶段一相同配置)|
|混合精度|FP16/BF16 （若非使用QLoRA，可开启混合精度节省显存）|FP16/BF16|
|梯度检查点|开启（减小长序列训练的显存占用）|开启（同左）|
|梯度累积|开启（如实际batch过小，通过梯度累积凑够有效批规模）|开启（同左）|

**表1：继续预训练阶段与指令微调阶段的训练参数建议。**

以上参数设置旨在在**保证效果**的前提下**提高训练效率**：

- **LoRA/QLoRA**：通过在每层添加少量可训练参数，显著减少训练开销，仅更新不到模型参数的1%即可达到全参数调优的效果[infohub.delltechnologies.com](https://infohub.delltechnologies.com/en-us/p/llama-2-efficient-fine-tuning-using-low-rank-adaptation-lora-on-single-gpu/#:~:text=LoRA%20is%20an%20efficient%20fine,optimizer%20states%20attached%20to%20them)[arxiv.org](https://arxiv.org/html/2508.02932v1#:~:text=tuning,as%20shown%20in%20Figure%C2%A01)。例如，在Qwen-7B上使用秩为64的LoRA时，仅需训练约3.4%的参数[arxiv.org](https://arxiv.org/html/2508.02932v1#:~:text=tuning,as%20shown%20in%20Figure%C2%A01)。QLoRA在此基础上将模型权重量化为4-bit，进一步将显存占用降低约33%，仅以约39%的额外训练时间为代价，性能几乎无损[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=backpropagation%2C%20QLoRA%20quantizes%20the%20pretrained,optimizers%20to%20handle%20memory%20spikes)[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=QLoRA%20with%204)。这使得65B模型在单张48GB卡上微调成为可能[arxiv.org](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is)。在本方案中，对8B模型采用LoRA/QLoRA，能轻松在单机单卡上完成训练任务。
    
- **学习率与调度**：继续预训练阶段采用略高的学习率，以较快融合新知识；指令微调阶段则适当降低学习率以精细调整生成风格。使用**预热+余弦退火**调度，有助于稳定收敛并避免过冲[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=Learning%20rate%20schedulers%20lower%20the,avoid%20overshooting%20the%20loss%20minima)。例如预热几百步至1e-4，然后缓慢降至接近0，既学习新知识又尽量不干扰原有知识分布。
    
- **批大小与轮次**：适度的有效批大小确保梯度估计稳定。《PLoRA》研究指出LoRA微调往往偏好**小批量**以取得更好效果[arxiv.org](https://arxiv.org/html/2508.02932v1#:~:text=Observation%20%234%3A%20LoRA%20Fine,We%20observe%20that%20LoRA)；因此我们在单卡显存有限时通过梯度累积实现一个中等批量（如16-32）。预训练语料重复训练2-3轮，以增大知识注入的深度；而指令数据集不宜轮次太多（1-2轮为宜），防止模型“背熟”答案影响泛化[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=When%20I%20increased%20the%20number,a%20decline%20in%20model%20performance)。
    
- **混合精度与梯度检查点**：这些技术在不影响最终效果的情况下节省显存，使长序列训练成为可能。特别地，如果使用QLoRA，需借助`bitsandbytes`库的4-bit优化器实现，梯度仍以16-bit计算。实验表明开启这些选项可以在不增加太多开销的情况下将7B模型LoRA微调的显存占用控制在15GB左右[infohub.delltechnologies.com](https://infohub.delltechnologies.com/en-us/p/llama-2-efficient-fine-tuning-using-low-rank-adaptation-lora-on-single-gpu/#:~:text=1,as%20illustrated%20in%20figure%202)[infohub.delltechnologies.com](https://infohub.delltechnologies.com/en-us/p/llama-2-efficient-fine-tuning-using-low-rank-adaptation-lora-on-single-gpu/#:~:text=Table%201,LoRA%20technique%20in%20our%20experiment)。


### 2. 训练阶段详解

#### 阶段一：领域继续预训练
**目标**：让模型**自主阅读**领域语料，融入计算机科学论文知识。训练任务为标准的自回归语言模型目标（next token prediction），在清洗后的论文纯文本上继续预训练若干步。
**初始化**：推荐使用**基础模型权重**作为初始化，而非已指令微调的聊天模型权重[arxiv.org](https://arxiv.org/html/2410.10739v1#:~:text=,tuning)。例如对Qwen-7B-Chat注入知识，有两种选择：
- **方案A**：从Qwen-7B基础模型出发，加载LoRA适配器，进行领域继续预训练，然后再进行指令微调恢复其聊天能力[arxiv.org](https://arxiv.org/html/2410.10739v1#:~:text=,tuning)。
- **方案B**：直接以Qwen-7B-Chat权重为底座，应用LoRA进行领域预训练。

研究表明，直接对指令微调后的模型继续预训练可能导致其**丧失部分指令遵循能力**[arxiv.org](https://arxiv.org/html/2410.10739v1#:~:text=With%20an%20assumption%20that%20the,tuned%20LLM)。因此本方案优选“方案A”，即采用基础模型进行领域训练，然后再通过第二阶段找回对话能力。若必须使用方案B（直接在Chat模型上训练），务必要**减小学习率和训练步长**防止过度偏移，并在阶段二额外使用指令数据重新校准模型的回答风格。

**训练过程**：使用上述表格中的参数配置，运行继续预训练：
- **数据准备**：将清洗后的论文文本打包成TFRecord或按行读入，随机混洗。可以使用HuggingFace `Dataset`的`TextDataset`加载数据，并指定`block_size=2048`按长度切分序列。如果论文较长，可将多篇论文串联成一个序列以充分利用上下文窗口（注意插入分隔符防止跨文档预测错误[arxiv.org](https://arxiv.org/html/2410.10739v1#:~:text=et%C2%A0al,to%20avoid%20cross%20article%20contamination)）。
- **模型加载**：通过HuggingFace Transformers加载基础模型（如`AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", load_in_4bit=True)`），加载对应的分词器（`AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)`）确保分词方式与预训练一致。
    
- **附加LoRA**：使用PEFT库，将LoRA适配器插入模型。例如：
    `from peft import LoraConfig, get_peft_model config = LoraConfig(r=16, lora_alpha=16, target_modules=["Wq","Wv"], lora_dropout=0.05, bias="none", task_type="CAUSAL_LM") model = get_peft_model(model, config)`
    
    （以上`target_modules`假定以Qwen结构的注意力投影层命名，可根据实际情况调整需要插入LoRA的层。）[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=6,Layers)[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms#:~:text=If%20we%20enable%20LoRA%20for,increase%20the%20modeling%20performance%20noticeably)
    
- **训练优化**：使用🤗 Transformers的`Trainer`或自行编写训练循环，配置AdamW优化器和学习率调度。如使用Trainer，可传入`TrainingArguments`指定上述超参数，例如`per_device_train_batch_size=2`（再通过`gradient_accumulation_steps=8`实现总batch=16），`learning_rate=1e-4`，`max_steps=XXX`或`num_train_epochs=3`，`warmup_steps=500`，`lr_scheduler_type="cosine"`, `fp16=True`等。
    
- **监控损失**：训练过程中监控验证集上的困惑度（如可留出一部分论文作为验证）。期望困惑度逐渐下降并趋于稳定。由于领域数据规模有限，不必追求训练到极低困惑度，以免过拟合细节。适可而止地结束阶段一（例如验证困惑度不再明显下降时）。
- **知识保持**：在阶段一结束时，模型已经通过LoRA适配器融入了新知识。需要注意，此时若使用基础模型进行训练，模型尚未具备指令对齐能力，需要进入阶段二；若直接用了Chat模型训练，模型可能在回答风格上出现漂移（变得更倾向陈述论文内容、忽略礼貌/安全等），这将在阶段二加以纠正。

**避免遗忘措施**：由于采用LoRA微调，基础模型的权重保持冻结，这**天然有助于防止灾难性遗忘**。原有的语言理解和常识知识基本保留在底座权重中，LoRA仅叠加新知识[infohub.delltechnologies.com](https://infohub.delltechnologies.com/en-us/p/llama-2-efficient-fine-tuning-using-low-rank-adaptation-lora-on-single-gpu/#:~:text=LoRA%20is%20an%20efficient%20fine,optimizer%20states%20attached%20to%20them)。因此，相比全参数微调，LoRA方式能更好地平衡新知识注入和旧知识保留。另外，如果担心模型回答风格受到影响，可在训练时少量混入原始指令数据或对话数据，让模型间隔看到一些用户-助手对话样本，提醒模型保持对话形式。不过一般来说，阶段二的存在足以恢复和增强模型的对话能力。

训练完成后，可以将LoRA适配器与基础模型权重合并（merge）以得到一个新的<font color="#ffc000">领域预训练模型</font>。也可以在不合并的情况下直接进入下阶段（由下阶段再应用新的LoRA），但合并后再微调可以简化流程（只有一组LoRA权重在第二阶段生效）。

### 阶段二：指令微调 (Supervised Fine-tuning)

**目标**：在注入领域知识的模型上**恢复并提升**其指令遵循能力，使模型能够以对话形式输出准确、有用的回答，针对文献问答、摘要等任务进行优化。该阶段相当于让模型“扮演”一个学术助手。
**训练数据**：使用前述构造的**领域指令数据集**（包含User-Assistant对）。这些数据是高质量的指令-响应对，可以视为领域专属的Instruction Tuning数据集，规模在数千左右。
**初始化**：采用阶段一得到的模型权重作为起点。如果阶段一使用的是基础模型+LoRA，现在可以：
- 将LoRA合并到基础模型，得到一个“领域知识增强”的模型权重，然后在此基础上附加新的LoRA适配器来进行指令微调；或者
- 保持阶段一的LoRA不变，新建一个LoRA用于指令微调（相当于双LoRA叠加）。为简便起见，建议前者——先将阶段一LoRA合并，使模型权重本身已经包含领域知识，然后再进行下一步微调[arxiv.org](https://arxiv.org/html/2410.10739v1#:~:text=Inspired%20by%20this%20idea%20of,on%20new%20skill%20that%20is)。

如果阶段一直接在Chat模型上微调的，也无需特殊处理，直接继续微调即可。

**训练过程**：总体流程与阶段一类似，不过任务从无监督变为有监督学习：

- **损失定义**：使用交叉熵监督训练，让模型以给定的User提示为条件，生成参考的Assistant回答。通常将User输入和参考Answer拼接为模型的输入序列，让模型在生成Answer部分的tokens时计算交叉熵损失。为了在多轮对话格式下准确计算损失，需对Prompt和Response部分进行mask，确保只计算助手回答部分的损失（Transformers的`GPT2LMHeadModel`可通过labels机制轻松实现这一点）。
    
- **参数设置**：采用表1中的指令微调列的参数。例如全局学习率设定为5e-5左右，训练1-2个epoch。因为数据量较小，可以设定一个固定步数如几千步并监控性能。Batch大小可较阶段一稍小，以确保每个batch的对话样本数量不多（对大模型，小批量反而效果更好[arxiv.org](https://arxiv.org/html/2508.02932v1#:~:text=Observation%20%234%3A%20LoRA%20Fine,We%20observe%20that%20LoRA)）。其余如混合精度、梯度累积等按照相同配置执行。
    
- **对话格式**：训练时要严格遵循模型期望的对话格式（例如Qwen-7B-Chat可能期待 **「<s>[USER] ... </s><s>[ASSISTANT] ... </s>」** 之类的格式）。应当利用模型提供的模版或special tokens来包装每条对话样本，使输入序列格式与模型在原始指令数据上训练时一致。如果缺少这方面信息，可参考官方范例或社区实践，确保**User提问**和**Assistant回答**被模型正确识别。这样训练才能有效强化模型的聊天能力。
    
- **监控和停止**：可以使用一组 held-out 的指令样本评估模型在生成回答方面的改善。如评估指标包括Rouge/LCS（针对摘要任务）、准确率（针对问答任务人工评判）等。一般1-2轮epoch后模型已经能很好地拟合这些指令数据。如发现训练损失持续降低但评测质量不提升，警惕过拟合，可提前停止训练。
    

**训练效果**：经过SFT，模型应能：

- **遵循指令**：无论用户提问方式如何，都能够以恰当的语气给出回应，包含所需的领域知识点。模型会更加**符合人类对话习惯**，比如回答先礼貌确认问题，再分点阐述技术细节等。这源于Chat模型原有对齐能力的恢复和加强[arxiv.org](https://arxiv.org/html/2410.10739v1#:~:text=Inspired%20by%20this%20idea%20of,on%20new%20skill%20that%20is)。
    
- **多任务能力**：能够胜任**文献问答**（回答学术问题）、**摘要生成**（概括论文内容）、**研究意义解读**（讨论论文影响）等多种任务要求。这在训练数据中已覆盖。模型有了领域知识储备，再加上指令调优，回答将既**专业**又**通俗易懂**，适合不同层次用户阅读。
    

训练完成后，若使用LoRA训练，可选择将LoRA合并到模型得到最终权重，或者在推理时通过指定LoRA权重加载来保持灵活性（例如可按需启用/关闭领域知识适配器）。

值得一提的是，还有一种**快速恢复指令能力**的方法：利用基础模型和原始Chat模型的权重差作为“指令残差”添加回模型[arxiv.org](https://arxiv.org/html/2410.10739v1#:~:text=Inspired%20by%20this%20idea%20of,on%20new%20skill%20that%20is)。具体来说，计算原始Qwen-7B-Chat权重与Qwen-7B基础模型权重的差值，将此差值加到阶段一训练后的模型上，即可在无需完整微调的情况下瞬间让模型具备原有的指令遵循能力[arxiv.org](https://arxiv.org/html/2410.10739v1#:~:text=Inspired%20by%20this%20idea%20of,on%20new%20skill%20that%20is)。这一技术基于文献【1】提出的“Instruction Residuals”，本方案中提供传统SFT方案已能满足需求，但了解这一技巧有助于日后进一步优化微调流程。


